from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json
import argparse
import os


def parse_args():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description="Calculate token probabilities for a given text file.")
    parser.add_argument("prompt_file", help="Path to the text file containing the prompt.")
    parser.add_argument("output_file", help="Path to the text file containing the model's output.")
    parser.add_argument("--output_dir", default="probabilities", help="Directory to save the output JSON file.")
    parser.add_argument("--model_name", default="meta-llama/Llama-3.1-8B-Instruct", help="Model to use for probability calculations.")
    return parser.parse_args()


def load_text_file(filepath, file_description):
    """Load text from a file with proper error handling."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"Error: {file_description} file not found at {filepath}")
        exit(1)
    except Exception as e:
        print(f"Error reading {file_description} file {filepath}: {e}")
        exit(1)

def load_model_and_tokenizer(model_name):
    """Load and initialize the model and tokenizer."""
    print(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    model.eval()
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")
    model.to(device)
    
    return model, tokenizer, device


def calculate_token_probabilities(model, tokenizer, device, prompt, output, top_k=6):
    """Calculate token probabilities for the output text given the prompt."""
    # Tokenize prompt and output separately
    prompt_encoding = tokenizer(prompt, return_tensors="pt")
    output_encoding = tokenizer(output, return_tensors="pt")
    
    prompt_ids = prompt_encoding["input_ids"][0].to(device)
    output_ids = output_encoding["input_ids"][0].to(device)
    
    # Concatenate for single forward pass
    all_ids = torch.cat([prompt_ids, output_ids]).unsqueeze(0)
    
    with torch.no_grad():
        logits = model(all_ids).logits  # [1, total_len, vocab_size]
    
    prompt_len = prompt_ids.size(0)
    vocab_size = tokenizer.vocab_size
    token_data = []
    
    for i, token_id in enumerate(output_ids):
        token_id = token_id.item()  # Convert to regular int
        
        # Get logits for position that predicts current token
        current_logits = logits[0, prompt_len + i - 1]
        probabilities = torch.softmax(current_logits, dim=-1)
        
        # Get probability and rank of actual token
        actual_prob = probabilities[token_id].item()
        
        # Get raw token string (no cleaning - preserve original data)
        token_str = tokenizer.decode([token_id])
        
        # Calculate rank efficiently
        sorted_indices = torch.argsort(probabilities, descending=True)
        actual_rank = (sorted_indices == token_id).nonzero(as_tuple=True)[0].item() + 1
        
        # Get top k alternatives (save raw tokens)
        top_k_probs, top_k_indices = torch.topk(probabilities, top_k)
        top_k_tokens = [
            tokenizer.decode([idx.item()])
            for idx in top_k_indices
        ]
        
        top_alternatives = [
            {"token": token, "probability": prob.item()}
            for token, prob in zip(top_k_tokens, top_k_probs)
        ]
        
        token_data.append({
            "token": token_str,
            "probability": actual_prob,
            "rank": actual_rank,
            "top_alternatives": top_alternatives
        })
    
    return token_data, vocab_size

def save_token_data(token_data, vocab_size, prompt, output_dir, output_file, model_name):
    """Save token probability data to JSON file."""
    if not os.path.exists(output_dir):
        print(f"Creating output directory: {output_dir}")
        os.makedirs(output_dir)
    
    base_filename = os.path.basename(output_file)
    json_filename = os.path.splitext(base_filename)[0] + ".json"
    output_json_path = os.path.join(output_dir, json_filename)
    
    try:
        with open(output_json_path, 'w', encoding='utf-8') as f:
            output_container = {
                "prompt": prompt,
                "vocab_size": vocab_size,
                "model_name": model_name,
                "tokens": token_data
            }
            json.dump(output_container, f, ensure_ascii=False, indent=2)
        print(f"Token data successfully saved to {output_json_path}")
    except Exception as e:
        print(f"Error saving token data to {output_json_path}: {e}")

def main():
    """Main function to orchestrate the probability calculation."""
    args = parse_args()
    
    # Load input files
    prompt = load_text_file(args.prompt_file, "Prompt")
    output = load_text_file(args.output_file, "Output")
    
    # Load model and tokenizer
    model, tokenizer, device = load_model_and_tokenizer(args.model_name)
    
    # Calculate token probabilities
    token_data, vocab_size = calculate_token_probabilities(model, tokenizer, device, prompt, output)
    
    # Save results
    save_token_data(token_data, vocab_size, prompt, args.output_dir, args.output_file, args.model_name)


if __name__ == "__main__":
    main()