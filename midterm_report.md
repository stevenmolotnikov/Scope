Perplex: An Interface for Token-level Investigation of LLM Outputs
Steven Molotnikov, Caden Juang
steven.molotnikov@gmail.com, kh4dien@gmail.com

This outline is here to help you write a useful midterm report. However, feel free to deviate from this structure if need be! We just ask that you write some sort of abstract and give preliminary results and methods that you used so far. 

Abstract
In 100–200 words, provide an overview of your research question and why it matters for addressing risks from advanced AI. Describe your methodological approach and any preliminary findings. If you're working on technical safety or security, mention the specific threat model, vector, or vulnerability you're addressing if possible. Keep this section accessible to readers outside your specific subfield, and avoid footnotes or citations here.
We introduce Perplex, a research interface for token-level investigation of large language models. We argue that token probabilities are the natural starting point for the analysis of AI behaviors as they reflect a model’s internal confidence and reveal alternative outputs at zero additional cost. Perplex supports core safety workflows such as hallucination detection (via real-time confidence drop alerts), model comparison (to investigate divergence between base and fine-tuned models), jailbreaking (via refusal vs. compliance probability shifts), and calibration evaluation (by parsing stated confidence against ground truth). In Perplex, researchers can generate completions in a chatbot UI, then interactively observe per-token log-probabilities, entropy, and top-k alternatives. With one click, they can inject counterfactual tokens, branch generations, or open deeper interpretability tools like LogitLens. Interfaces for AI safety research remain vastly underdeveloped. We believe thoughtful tooling can compress hours of manual inspection into minutes of interactive exploration, enabling rapid iteration, insight, and collaboration. We hope Perplex becomes a foundational tool that helps researchers triage where to look, identify failure modes, and accumulate deeper, structured understanding of model behavior.
Introduction
Describe the specific problem or question you're investigating and explain why it matters. Your introduction should make clear what gap in existing work you're trying to fill or what risk you're trying to address.
Help readers understand how your work relates to the broader landscape. You don't need a comprehensive literature review at this stage, but situate your project relative to existing approaches or prior work. Finally, articulate what success would look like for your project. What would you need to demonstrate or discover for this research to be valuable?
Aim for 100–350 words.
LLM safety research has a data accessibility problem. When investigating why alignment techniques fail, where models hallucinate, or how fine-tuning changes behavior, researchers need to understand token-level decision-making—yet the probability distributions that reveal this are locked away. Most interfaces show only final outputs, hiding the model's uncertainty and alternatives considered at each step. Even when probabilities are accessible via API, they arrive as raw JSON requiring custom scripts to extract and visualize—a barrier that turns quick investigations into multi-hour engineering tasks.

This gap is particularly costly for safety work. Researchers studying jailbreaks manually parse logprobs to find refusal-to-compliance shifts. Those tracking hallucinations write one-off scripts to flag confidence drops. Teams comparing base versus fine-tuned models build bespoke pipelines to identify behavioral divergence. The underlying data exists, but extracting insight from it requires reinventing infrastructure rather than focusing on the research question.

Perplex closes this gap by making token probabilities a first-class interface primitive. We provide a ChatGPT-style UI where every token displays its log-probability, entropy, and top-k alternatives on hover. Researchers can inject counterfactual tokens and instantly see how generations change, compare models side-by-side to visualize divergence points, or set confidence thresholds that alert when the model becomes uncertain. This transforms probability data from an API artifact into an interactive investigation tool—enabling the same rapid hypothesis testing that characterizes effective empirical research in other fields.

Our work complements deep interpretability tools (TransformerLens, circuit analysis) by helping researchers identify where to apply them. Scanning hundreds of tokens for anomalous probability patterns is fast; mechanistic analysis of specific attention heads is slow but precise. Perplex serves as the triage layer that makes expensive tools more effective.

Success means: (1) researchers can identify failure modes in minutes rather than hours, (2) investigations become reproducible through saved sessions and annotations, and (3) probability-first workflows enable new research questions that were previously too tedious to pursue systematically.
Methodology
Describe your research approach clearly enough that someone could understand what you're doing and roughly replicate it. For empirical work, include your experimental setup, systems or datasets, and evaluation metrics. For theoretical work, outline your framework, key assumptions, and approach to arguments or proofs. For policy research, describe your research design, data sources, and analytical approach. 
Aim for 200–500 words.
Our approach began with a grounded exploration of how token-level probability data has been used across empirical studies in alignment, interpretability, and decoding research. We reviewed a range of prior work where researchers manually extracted log-probabilities or entropy to detect hallucinations, track learning progress, measure calibration, or analyze model refusal behavior. By closely examining these case studies, we identified a recurring bottleneck: although the underlying token-level data is available through model APIs or logs, it is difficult to access, interpret, and act upon in practice—requiring custom scripts, manual parsing, and ad hoc tooling.
To translate these patterns into interface design, we adopted a user-centered prototyping strategy: for each paper or use case (e.g., jailbreaking, hallucination detection, model comparison), we identified the specific analysis questions being asked and then brainstormed minimal interface affordances that would make those workflows faster and more systematic. For instance, hallucination work often involved looking for sharp confidence drops or anomalously high entropy, which we translated into timeline-based entropy heatmaps and real-time alerting thresholds. Work comparing pre-trained and fine-tuned models inspired a side-by-side comparison mode with token-level divergence tracking and shared tokenizer alignment.
We synthesized these individual features into a unified interface specification with modular analysis modes. Each mode—Default, Hallucination, Model Comparison, Prefilling, Calibration—reconfigures the interface to foreground specific aspects of the probability data relevant to that task. The core infrastructure includes a ChatGPT-style UI for prompt-response interaction, a token overlay layer showing log-probabilities and top-k alternatives, and a context menu system for branching generations, forcing tokens, and launching interpretability tools (e.g. LogitLens).
We adopted an iterative specification process: as new workflows or risks (e.g., contrastive decoding failure modes) were considered, we extended the design by integrating new viewing modes or controls while preserving a minimal default experience. Our guiding principle throughout was progressive disclosure: users begin with a lightweight view of model uncertainty and can opt into deeper investigation tools as needed.
This methodology, although speculative in nature, is designed to produce a high-leverage research tool that can be evaluated through both qualitative feedback and downstream productivity metrics. Once implemented, our evaluation will involve case study replication (e.g., reconstructing known jailbreaks or hallucinations) and user studies measuring investigation time, reproducibility, and insight density across Perplex vs. baseline log analysis.
By reverse-engineering real research workflows into usable interface primitives, we aim to validate Perplex not just as a visualization tool, but as a general-purpose instrument for empirical AI safety investigation.
Preliminary Results
Present what you've found so far, using subheadings if helpful. For empirical work, the focus is typically on showing quantitative findings with visualizations. For theoretical work, it’s common to focus on key insights or partial results. For policy research, present initial findings from your analysis. It's normal for midterm results to be incomplete. 
Aim for 200–500 words.
At midterm, we have implemented a working prototype (Old_Perplex) that demonstrates the core feasibility of token-level probability visualization and validates several design assumptions about how such interfaces can accelerate safety investigations. The prototype is a Flask-based web application that interfaces with HuggingFace transformer models locally, enabling full control over probability extraction. The system successfully extracts per-token log-probabilities, ranks, and top-k alternatives for any prompt-output pair and presents this data through an interactive web interface.
The visualization layer uses color-coding to map probabilities to a red-yellow-green gradient, making low-confidence tokens immediately visible without requiring statistical expertise. Users can hover over any token to see exact probability values, rank within the vocabulary, and the top three alternative tokens the model considered. We implemented both probability-based and rank-based visualization modes, with configurable thresholds for flagging suspicious tokens. The interface automatically highlights tokens that fall below probability thresholds or exceed rank thresholds, providing at-a-glance identification of out-of-distribution behavior. This supports the token-level control strategy described in our motivation: detecting when an untrusted model generates tokens that would be implausible under a trusted model with only the benign task.
We also implemented a differential probability analysis mode, where the same output can be evaluated under two different prompts—for instance, comparing token probabilities when a model receives only a benign primary task versus when it receives both primary and hidden malicious side tasks. This allows direct measurement of how prompt context affects token-level plausibility, which is central to control-based safety approaches. Additionally, the system calculates perplexity for each analyzed sequence, providing a scalar metric to complement token-level inspection.
Challenges & Open Questions
Be honest about obstacles you've encountered: technical difficulties, resource constraints, or conceptual challenges. Note any areas where you're still figuring things out or where the path forward is uncertain. 
Optional but encouraged. Aim for 100–300 words.
Several significant challenges have emerged during prototyping and design. First, there is a substantial gap between our current implementation (Old_Perplex) and our full specification. Old_Perplex successfully demonstrates basic probability visualization but lacks the interactive manipulation features central to our vision—specifically token injection, branching, real-time generation with streaming probability updates, and the mode-based interface reconfiguration system. Bridging this gap requires architectural decisions we're still evaluating: should we rebuild from scratch with a modern React frontend and proper state management, or iterate on the Flask prototype until feature parity is reached?
Second, we face a tension between local inference and API-based generation. Old_Perplex uses local HuggingFace models, giving full control over probability extraction but imposing significant computational requirements (model loading times, GPU memory constraints) that limit which models can be investigated. Moving to API-based generation would improve accessibility and support larger proprietary models, but many providers (especially for open-source models via inference services) don't reliably expose logprobs. Anthropic and OpenAI do, but with rate limits and cost considerations that could constrain user testing.
Third, we face an evaluation challenge: how do we rigorously demonstrate that probability-first interfaces materially improve safety research? Our current results rely on informal observations and design intuition. We lack ground-truth benchmarks for "investigation velocity" or "insight discovery rate." User studies will be essential, but recruiting active safety researchers, getting them to integrate an unfamiliar tool into real workflows, and collecting meaningful comparative data is nontrivial. We're uncertain whether lab-based controlled experiments or longitudinal field deployments will provide more compelling evidence.
Finally, we face an open question about impact scope: many of the workflows we designed support existing research paradigms (replicating known jailbreak investigations, hallucination detection, model comparison). How does Perplex enable fundamentally new research directions rather than just accelerating old ones? Will mechanistic interpretability researchers—who already use sophisticated tools like TransformerLens—find probability patterns useful, or will they see this as too surface-level? These questions will only be answered through real-world usage and feedback.
Next Steps 
Highlight what you will focus on and what you will try to accomplish during the next half of the program. What will you prioritize? What might you deprioritize or cut if needed?
Aim for 150–400 words.
For the second half, we'll build the minimal viable version from our "light spec" and validate through user testing. We've identified three core priorities based on Old_Perplex experience: First, implement a ChatGPT-style interface with streaming generation and real-time probability overlays. This requires architectural decisions (React + FastAPI backend) and inference strategy (hybrid: OpenAI/Anthropic APIs for accessibility, optional vLLM for local models). The key interaction—hover for probabilities, click for top alternatives—must be polished and responsive.

Second, add token interference (click, select alternative, regenerate from that point) and model comparison (side-by-side generation with divergence highlighting). These enable counterfactual exploration and behavior comparison, which are central to safety investigations but currently require extensive custom tooling. Third, implement prefilling support (input field for first assistant tokens) and basic session management (save/load investigations).

We're deprioritizing all specialized modes beyond Default (Hallucination, Calibration, Contrastive Decoding), LogitLens integration, batch processing, and advanced export features. The goal is shipping a working tool researchers can use, gathering feedback, and iterating based on real usage rather than building speculative features. If time permits, we'll conduct informal testing with 3-5 researchers to collect qualitative feedback and identify highest-value features for post-program development.
Works Cited/Bibliography
The reference page lists all sources used for your project, including those cited in the review and methods sections. Ensure the citation style is consistent throughout.



Perplex: An Interface for Token-level Investigation of LLM Outputs
Steven Molotnikov, Caden Juang
steven.molotnikov@gmail.com, kh4dien@gmail.com

This outline is here to help you write a useful midterm report. However, feel free to deviate from this structure if need be! We just ask that you write some sort of abstract and give preliminary results and methods that you used so far. 

Abstract
In 100–200 words, provide an overview of your research question and why it matters for addressing risks from advanced AI. Describe your methodological approach and any preliminary findings. If you're working on technical safety or security, mention the specific threat model, vector, or vulnerability you're addressing if possible. Keep this section accessible to readers outside your specific subfield, and avoid footnotes or citations here.
We introduce Perplex, a research interface for token-level investigation of large language models. We argue that token probabilities are the natural starting point for the analysis of AI behaviors as they reflect a model’s internal confidence and reveal alternative outputs at zero additional cost. Perplex supports core safety workflows such as hallucination detection (via real-time confidence drop alerts), model comparison (to investigate divergence between base and fine-tuned models), jailbreaking (via refusal vs. compliance probability shifts), and calibration evaluation (by parsing stated confidence against ground truth). In Perplex, researchers can generate completions in a chatbot UI, then interactively observe per-token log-probabilities, entropy, and top-k alternatives. With one click, they can inject counterfactual tokens, branch generations, or open deeper interpretability tools like LogitLens. Interfaces for AI safety research remain vastly underdeveloped. We believe thoughtful tooling can compress hours of manual inspection into minutes of interactive exploration, enabling rapid iteration, insight, and collaboration. We hope Perplex becomes a foundational tool that helps researchers triage where to look, identify failure modes, and accumulate deeper, structured understanding of model behavior.
Introduction
Describe the specific problem or question you're investigating and explain why it matters. Your introduction should make clear what gap in existing work you're trying to fill or what risk you're trying to address.
Help readers understand how your work relates to the broader landscape. You don't need a comprehensive literature review at this stage, but situate your project relative to existing approaches or prior work. Finally, articulate what success would look like for your project. What would you need to demonstrate or discover for this research to be valuable?
Aim for 100–350 words.
LLM safety research has a data accessibility problem. When investigating why alignment techniques fail, where models hallucinate, or how fine-tuning changes behavior, researchers need to understand token-level decision-making—yet the probability distributions that reveal this are locked away. Most interfaces show only final outputs, hiding the model's uncertainty and alternatives considered at each step. Even when probabilities are accessible via API, they arrive as raw JSON requiring custom scripts to extract and visualize—a barrier that turns quick investigations into multi-hour engineering tasks.

This gap is particularly costly for safety work. Researchers studying jailbreaks manually parse logprobs to find refusal-to-compliance shifts. Those tracking hallucinations write one-off scripts to flag confidence drops. Teams comparing base versus fine-tuned models build bespoke pipelines to identify behavioral divergence. The underlying data exists, but extracting insight from it requires reinventing infrastructure rather than focusing on the research question.

Perplex closes this gap by making token probabilities a first-class interface primitive. We provide a ChatGPT-style UI where every token displays its log-probability, entropy, and top-k alternatives on hover. Researchers can inject counterfactual tokens and instantly see how generations change, compare models side-by-side to visualize divergence points, or set confidence thresholds that alert when the model becomes uncertain. This transforms probability data from an API artifact into an interactive investigation tool—enabling the same rapid hypothesis testing that characterizes effective empirical research in other fields.

Our work complements deep interpretability tools (TransformerLens, circuit analysis) by helping researchers identify where to apply them. Scanning hundreds of tokens for anomalous probability patterns is fast; mechanistic analysis of specific attention heads is slow but precise. Perplex serves as the triage layer that makes expensive tools more effective.

Success means: (1) researchers can identify failure modes in minutes rather than hours, (2) investigations become reproducible through saved sessions and annotations, and (3) probability-first workflows enable new research questions that were previously too tedious to pursue systematically.
Methodology
Describe your research approach clearly enough that someone could understand what you're doing and roughly replicate it. For empirical work, include your experimental setup, systems or datasets, and evaluation metrics. For theoretical work, outline your framework, key assumptions, and approach to arguments or proofs. For policy research, describe your research design, data sources, and analytical approach. 
Aim for 200–500 words.
Our approach began with a grounded exploration of how token-level probability data has been used across empirical studies in alignment, interpretability, and decoding research. We reviewed a range of prior work where researchers manually extracted log-probabilities or entropy to detect hallucinations, track learning progress, measure calibration, or analyze model refusal behavior. By closely examining these case studies, we identified a recurring bottleneck: although the underlying token-level data is available through model APIs or logs, it is difficult to access, interpret, and act upon in practice—requiring custom scripts, manual parsing, and ad hoc tooling.
To translate these patterns into interface design, we adopted a user-centered prototyping strategy: for each paper or use case (e.g., jailbreaking, hallucination detection, model comparison), we identified the specific analysis questions being asked and then brainstormed minimal interface affordances that would make those workflows faster and more systematic. For instance, hallucination work often involved looking for sharp confidence drops or anomalously high entropy, which we translated into timeline-based entropy heatmaps and real-time alerting thresholds. Work comparing pre-trained and fine-tuned models inspired a side-by-side comparison mode with token-level divergence tracking and shared tokenizer alignment.
We synthesized these individual features into a unified interface specification with modular analysis modes. Each mode—Default, Hallucination, Model Comparison, Prefilling, Calibration—reconfigures the interface to foreground specific aspects of the probability data relevant to that task. The core infrastructure includes a ChatGPT-style UI for prompt-response interaction, a token overlay layer showing log-probabilities and top-k alternatives, and a context menu system for branching generations, forcing tokens, and launching interpretability tools (e.g. LogitLens).
We adopted an iterative specification process: as new workflows or risks (e.g., contrastive decoding failure modes) were considered, we extended the design by integrating new viewing modes or controls while preserving a minimal default experience. Our guiding principle throughout was progressive disclosure: users begin with a lightweight view of model uncertainty and can opt into deeper investigation tools as needed.
This methodology, although speculative in nature, is designed to produce a high-leverage research tool that can be evaluated through both qualitative feedback and downstream productivity metrics. Once implemented, our evaluation will involve case study replication (e.g., reconstructing known jailbreaks or hallucinations) and user studies measuring investigation time, reproducibility, and insight density across Perplex vs. baseline log analysis.
By reverse-engineering real research workflows into usable interface primitives, we aim to validate Perplex not just as a visualization tool, but as a general-purpose instrument for empirical AI safety investigation.
Preliminary Results
Present what you've found so far, using subheadings if helpful. For empirical work, the focus is typically on showing quantitative findings with visualizations. For theoretical work, it’s common to focus on key insights or partial results. For policy research, present initial findings from your analysis. It's normal for midterm results to be incomplete. 
Aim for 200–500 words.
At midterm, we have implemented a working prototype (Old_Perplex) that demonstrates the core feasibility of token-level probability visualization and validates several design assumptions about how such interfaces can accelerate safety investigations. The prototype is a Flask-based web application that interfaces with HuggingFace transformer models locally, enabling full control over probability extraction. The system successfully extracts per-token log-probabilities, ranks, and top-k alternatives for any prompt-output pair and presents this data through an interactive web interface.
The visualization layer uses color-coding to map probabilities to a red-yellow-green gradient, making low-confidence tokens immediately visible without requiring statistical expertise. Users can hover over any token to see exact probability values, rank within the vocabulary, and the top three alternative tokens the model considered. We implemented both probability-based and rank-based visualization modes, with configurable thresholds for flagging suspicious tokens. The interface automatically highlights tokens that fall below probability thresholds or exceed rank thresholds, providing at-a-glance identification of out-of-distribution behavior. This supports the token-level control strategy described in our motivation: detecting when an untrusted model generates tokens that would be implausible under a trusted model with only the benign task.
We also implemented a differential probability analysis mode, where the same output can be evaluated under two different prompts—for instance, comparing token probabilities when a model receives only a benign primary task versus when it receives both primary and hidden malicious side tasks. This allows direct measurement of how prompt context affects token-level plausibility, which is central to control-based safety approaches. Additionally, the system calculates perplexity for each analyzed sequence, providing a scalar metric to complement token-level inspection.
Challenges & Open Questions
Be honest about obstacles you've encountered: technical difficulties, resource constraints, or conceptual challenges. Note any areas where you're still figuring things out or where the path forward is uncertain. 
Optional but encouraged. Aim for 100–300 words.
Several significant challenges have emerged during prototyping and design. First, there is a substantial gap between our current implementation (Old_Perplex) and our full specification. Old_Perplex successfully demonstrates basic probability visualization but lacks the interactive manipulation features central to our vision—specifically token injection, branching, real-time generation with streaming probability updates, and the mode-based interface reconfiguration system. Bridging this gap requires architectural decisions we're still evaluating: should we rebuild from scratch with a modern React frontend and proper state management, or iterate on the Flask prototype until feature parity is reached?
Second, we face a tension between local inference and API-based generation. Old_Perplex uses local HuggingFace models, giving full control over probability extraction but imposing significant computational requirements (model loading times, GPU memory constraints) that limit which models can be investigated. Moving to API-based generation would improve accessibility and support larger proprietary models, but many providers (especially for open-source models via inference services) don't reliably expose logprobs. Anthropic and OpenAI do, but with rate limits and cost considerations that could constrain user testing.
Third, we face an evaluation challenge: how do we rigorously demonstrate that probability-first interfaces materially improve safety research? Our current results rely on informal observations and design intuition. We lack ground-truth benchmarks for "investigation velocity" or "insight discovery rate." User studies will be essential, but recruiting active safety researchers, getting them to integrate an unfamiliar tool into real workflows, and collecting meaningful comparative data is nontrivial. We're uncertain whether lab-based controlled experiments or longitudinal field deployments will provide more compelling evidence.
Finally, we face an open question about impact scope: many of the workflows we designed support existing research paradigms (replicating known jailbreak investigations, hallucination detection, model comparison). How does Perplex enable fundamentally new research directions rather than just accelerating old ones? Will mechanistic interpretability researchers—who already use sophisticated tools like TransformerLens—find probability patterns useful, or will they see this as too surface-level? These questions will only be answered through real-world usage and feedback.
Next Steps 
Highlight what you will focus on and what you will try to accomplish during the next half of the program. What will you prioritize? What might you deprioritize or cut if needed?
Aim for 150–400 words.
For the second half, we'll build the minimal viable version from our "light spec" and validate through user testing. We've identified three core priorities based on Old_Perplex experience: First, implement a ChatGPT-style interface with streaming generation and real-time probability overlays. This requires architectural decisions (React + FastAPI backend) and inference strategy (hybrid: OpenAI/Anthropic APIs for accessibility, optional vLLM for local models). The key interaction—hover for probabilities, click for top alternatives—must be polished and responsive.

Second, add token interference (click, select alternative, regenerate from that point) and model comparison (side-by-side generation with divergence highlighting). These enable counterfactual exploration and behavior comparison, which are central to safety investigations but currently require extensive custom tooling. Third, implement prefilling support (input field for first assistant tokens) and basic session management (save/load investigations).

We're deprioritizing all specialized modes beyond Default (Hallucination, Calibration, Contrastive Decoding), LogitLens integration, batch processing, and advanced export features. The goal is shipping a working tool researchers can use, gathering feedback, and iterating based on real usage rather than building speculative features. If time permits, we'll conduct informal testing with 3-5 researchers to collect qualitative feedback and identify highest-value features for post-program development.
Works Cited/Bibliography
The reference page lists all sources used for your project, including those cited in the review and methods sections. Ensure the citation style is consistent throughout.



