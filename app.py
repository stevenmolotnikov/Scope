from flask import Flask, render_template, url_for, request, redirect, flash, session, Response, stream_with_context
import json
import os
import math
import datetime
import re
import secrets
import time

app = Flask(__name__)
app.secret_key = secrets.token_hex(16)

# Configuration
OUTPUT_DATA_DIR = "probabilities"
DEFAULT_PROB_THRESHOLD = 1e-4
DEFAULT_RANK_THRESHOLD = 100
DEFAULT_MAX_RANK_SCALING = 100
DEFAULT_MIN_PROB_SCALING = 0.0
DEFAULT_MODEL = "meta-llama/Llama-3.1-8B-Instruct"

# Global model cache to avoid reloading
MODEL_CACHE = {}

def get_color_for_prob(probability, min_prob_for_scaling=0.0):
    """Maps probability (0-1) to an HSL color string.
    Low prob -> Red (0°), High prob -> Green (120°)
    Optionally compress the scale by setting min_prob_for_scaling > 0.
    """
    if probability is None or not (0 <= probability <= 1):
        return "hsl(0, 0%, 80%)"  # Grey for invalid
    
    # Optional: compress scale if min_prob_for_scaling is set
    if min_prob_for_scaling > 0:
        probability = max(probability, min_prob_for_scaling)
        # Rescale from [min, 1] to [0, 1]
        probability = (probability - min_prob_for_scaling) / (1 - min_prob_for_scaling)
    
    # Simple linear mapping: 0 -> red (0°), 1 -> green (120°)
    hue = 120 * probability
    
    return f"hsl({hue:.1f}, 70%, 60%)"


def get_color_for_rank(rank, vocab_size, max_rank_for_scaling=100):
    """Maps rank (1-vocab_size) to an HSL color string.
    Low rank (good) -> Green (120°), High rank (bad) -> Red (0°)
    Clamps at max_rank_for_scaling to avoid everything being red.
    """
    if rank is None or vocab_size is None or rank < 1:
        return "hsl(0, 0%, 80%)"  # Grey for invalid
    
    # Clamp rank to max_rank_for_scaling
    rank = min(rank, max_rank_for_scaling)
    
    # Convert to 0-1 scale: rank 1 -> 1.0 (best), rank max -> 0.0 (worst)
    normalized = 1.0 - ((rank - 1) / (max_rank_for_scaling - 1))
    
    # Map to hue: 0 -> red (0°), 1 -> green (120°)
    hue = 120 * normalized
    
    return f"hsl({hue:.1f}, 70%, 60%)"


def scan_available_files():
    """Scan for available JSON files in the output directory."""
    try:
        if not os.path.exists(OUTPUT_DATA_DIR):
            print(f"Creating data directory: {OUTPUT_DATA_DIR}")
            os.makedirs(OUTPUT_DATA_DIR)
        available_files = [f for f in os.listdir(OUTPUT_DATA_DIR) if f.endswith('.json')]
        available_files.sort()
        return available_files, None
    except Exception as e:
        return [], f"Error scanning directory {OUTPUT_DATA_DIR}: {e}"


def determine_file_to_load(available_files, selected_file):
    """Determine which file to load based on available files and selection."""
    if selected_file and selected_file in available_files:
        return os.path.join(OUTPUT_DATA_DIR, selected_file), selected_file
    elif not selected_file and available_files:
        # Load first file if available and none selected
        first_file = available_files[0]
        return os.path.join(OUTPUT_DATA_DIR, first_file), first_file
    return None, selected_file


def load_token_data(file_path):
    """Load and parse token data from JSON file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return (data.get("prompt", "Prompt not found in JSON data."), 
                data.get("tokens", []), 
                data.get("vocab_size"), 
                data.get("model_name"),
                None)
    except FileNotFoundError:
        return None, [], None, None, f"File not found at {file_path}"
    except json.JSONDecodeError:
        return None, [], None, None, f"Error decoding {file_path}. Is it valid JSON?"
    except Exception as e:
        return None, [], None, None, f"An unexpected error occurred while processing {file_path}: {e}"


def clean_tokenizer_artifacts(token):
    """Clean up tokenizer artifacts from existing JSON data."""
    if not isinstance(token, str):
        token = str(token)
    
    # Remove common special tokens
    special_tokens_to_remove = [
        '<|begin_of_text|>', '<|end_of_text|>', 
        '<|start_header_id|>', '<|end_header_id|>',
        '<s>', '</s>', '<pad>', '<unk>'
    ]
    
    for special_token in special_tokens_to_remove:
        token = token.replace(special_token, '')
    
    # Clean up GPT-style tokenizer artifacts
    token = token.replace('Ġ', ' ')  # GPT space marker
    token = token.replace('Ċ', '\n')  # GPT newline marker
    token = token.replace('ĉ', '\t')  # GPT tab marker
    
    # Clean up other common artifacts
    token = token.replace('▁', ' ')  # SentencePiece space marker
    token = token.replace('<0x0A>', '\n')  # Hex newline
    token = token.replace('<0x20>', ' ')  # Hex space
    
    return token


def format_token_for_html(token):
    """Clean tokenizer artifacts and prepare for HTML display while making whitespace visible."""
    if not isinstance(token, str):
        token = str(token)
    
    # First clean tokenizer artifacts
    clean_token = clean_tokenizer_artifacts(token)
    
    # HTML escape the clean token
    escaped_token = clean_token.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
    
    # Make whitespace tokens visible while preserving their function
    if clean_token == ' ':
        return '·'  # Middle dot for space - visible but subtle
    elif clean_token == '\n':
        return '↵'  # Return symbol for newline
    elif clean_token == '\t':
        return '→'  # Right arrow for tab
    elif clean_token.isspace() and len(clean_token) > 1:
        # Multiple spaces or mixed whitespace - just show the visual representation
        result = ''
        for char in clean_token:
            if char == ' ':
                result += '·'
            elif char == '\n':
                result += '↵'
            elif char == '\t':
                result += '→'
            else:
                result += char  # Any other whitespace
        return result
    
    return escaped_token


def process_token_data(token_input_list, vocab_size, threshold_mode, max_rank_for_scaling, min_prob_for_scaling):
    """Process raw token data into formatted data for display."""
    processed_token_data = []
    
    for idx, item in enumerate(token_input_list):
        prob = item.get('probability')
        token = item.get('token', '')
        rank = item.get('rank')
        top_alternatives = item.get('top_alternatives', [])
        
        # Simple token processing - just format for HTML display
        display_token = format_token_for_html(token)
        token_class = determine_token_class(token)
        
        # Process alternatives for HTML display
        processed_alternatives = [
            {'token': format_token_for_html(alt.get('token', '')), 'probability': alt.get('probability')}
            for alt in top_alternatives
        ]
        
        # Choose color based on current mode
        token_color = (get_color_for_rank(rank, vocab_size, max_rank_for_scaling) 
                      if threshold_mode == 'rank' 
                      else get_color_for_prob(prob, min_prob_for_scaling))
        
        processed_item = {
            'probability': prob,
            'rank': rank,
            'vocab_size': vocab_size,
            'token': token,
            'color': token_color,
            'display_token': display_token,
            'token_class': token_class,
            'original_index': idx,
            'output_token_index': idx,
            'top_alternatives': processed_alternatives
        }
        processed_token_data.append(processed_item)
    
    return processed_token_data


def determine_token_class(token):
    """Determine CSS class for special tokens after cleaning artifacts."""
    # Clean the token first to check its actual content
    clean_token = clean_tokenizer_artifacts(token)
    
    if '\n' in clean_token or '\r' in clean_token:
        return "newline-token"
    elif clean_token.isspace() and clean_token != '':
        return "space-token"
    return ""


def apply_threshold_filtering(processed_token_data, threshold_mode, prob_threshold, rank_threshold):
    """Apply threshold filtering to highlight tokens."""
    if threshold_mode == 'rank':
        lowest_probs = [token for token in processed_token_data 
                       if token.get('rank') is not None and token['rank'] >= rank_threshold]
        lowest_probs.sort(key=lambda x: x['rank'], reverse=True)
    else:
        lowest_probs = [token for token in processed_token_data 
                       if token['probability'] is not None and token['probability'] < prob_threshold]
        lowest_probs.sort(key=lambda x: x['probability'])
    
    # Mark tokens that meet threshold criteria
    lowest_prob_indices = {item['original_index'] for item in lowest_probs}
    for item in processed_token_data:
        item['is_lowest_prob'] = (item['original_index'] in lowest_prob_indices)
    
    return processed_token_data, lowest_probs


def load_model_and_tokenizer(model_name):
    """Load and cache model and tokenizer."""
    # Lazy import - only load these heavy libraries when needed
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    if model_name in MODEL_CACHE:
        return MODEL_CACHE[model_name]
    
    print(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Ensure pad token is set (some models don't have it by default)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print(f"Set pad_token to eos_token: {tokenizer.eos_token}")
    
    # Print tokenizer info
    print(f"Tokenizer: {tokenizer.__class__.__name__}")
    print(f"Has chat template: {tokenizer.chat_template is not None}")
    if tokenizer.chat_template:
        print(f"Chat template preview: {tokenizer.chat_template[:100]}...")
    
    model = AutoModelForCausalLM.from_pretrained(model_name)
    model.eval()
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")
    model.to(device)
    
    MODEL_CACHE[model_name] = (model, tokenizer, device)
    return model, tokenizer, device


def calculate_token_probabilities(model, tokenizer, device, prompt, output, top_k=3):
    """Calculate token probabilities for the output text given the prompt."""
    # Lazy import - only load when needed
    import torch
    
    # Tokenize prompt and output separately
    prompt_encoding = tokenizer(prompt, return_tensors="pt")
    output_encoding = tokenizer(output, return_tensors="pt")
    
    prompt_ids = prompt_encoding["input_ids"][0].to(device)
    output_ids = output_encoding["input_ids"][0].to(device)
    
    # Concatenate for single forward pass
    all_ids = torch.cat([prompt_ids, output_ids]).unsqueeze(0)
    
    with torch.no_grad():
        logits = model(all_ids).logits  # [1, total_len, vocab_size]
    
    prompt_len = prompt_ids.size(0)
    vocab_size = tokenizer.vocab_size
    token_data = []
    
    for i, token_id in enumerate(output_ids):
        token_id = token_id.item()  # Convert to regular int
        
        # Get logits for position that predicts current token
        current_logits = logits[0, prompt_len + i - 1]
        probabilities = torch.softmax(current_logits, dim=-1)
        
        # Get probability and rank of actual token
        actual_prob = probabilities[token_id].item()
        
        # Get raw token string (no cleaning - preserve original data)
        token_str = tokenizer.decode([token_id])
        
        # Calculate rank efficiently
        sorted_indices = torch.argsort(probabilities, descending=True)
        actual_rank = (sorted_indices == token_id).nonzero(as_tuple=True)[0].item() + 1
        
        # Get top k alternatives (save raw tokens)
        top_k_probs, top_k_indices = torch.topk(probabilities, top_k)
        top_k_tokens = [
            tokenizer.decode([idx.item()])
            for idx in top_k_indices
        ]
        
        top_alternatives = [
            {"token": token, "probability": prob.item()}
            for token, prob in zip(top_k_tokens, top_k_probs)
        ]
        
        token_data.append({
            "token": token_str,
            "probability": actual_prob,
            "rank": actual_rank,
            "top_alternatives": top_alternatives
        })
    
    return token_data, vocab_size


def generate_streaming_tokens(model, tokenizer, device, messages, max_new_tokens=512, top_k=3):
    """Generate tokens one at a time with probabilities using streaming.
    
    Args:
        messages: List of message dicts with 'role' and 'content' keys
                  e.g., [{"role": "user", "content": "Hello!"}]
    """
    import torch
    
    # Apply chat template to format messages correctly for the model
    if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template is not None:
        # Use the model's chat template
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        print(f"Using chat template. Formatted prompt: {formatted_prompt[:200]}...")
        input_ids = tokenizer.encode(formatted_prompt, return_tensors="pt").to(device)
    else:
        # Fallback: just use the last user message
        print("No chat template available, using raw message")
        prompt_text = messages[-1]['content'] if messages else ""
        input_ids = tokenizer.encode(prompt_text, return_tensors="pt").to(device)
    
    vocab_size = tokenizer.vocab_size
    
    # Generate tokens one by one
    generated_ids = input_ids.clone()
    
    for _ in range(max_new_tokens):
        with torch.no_grad():
            outputs = model(generated_ids)
            logits = outputs.logits[:, -1, :]  # Get logits for the last token
            
        # Calculate probabilities
        probabilities = torch.softmax(logits, dim=-1)
        
        # Sample next token (greedy for now)
        next_token_id = torch.argmax(probabilities, dim=-1).item()
        
        # Get probability and rank of selected token
        token_prob = probabilities[0, next_token_id].item()
        
        # Calculate rank
        sorted_indices = torch.argsort(probabilities[0], descending=True)
        token_rank = (sorted_indices == next_token_id).nonzero(as_tuple=True)[0].item() + 1
        
        # Get top k alternatives
        top_k_probs, top_k_indices = torch.topk(probabilities[0], top_k)
        top_k_tokens = [
            tokenizer.decode([idx.item()])
            for idx in top_k_indices
        ]
        
        top_alternatives = [
            {"token": token, "probability": prob.item()}
            for token, prob in zip(top_k_tokens, top_k_probs)
        ]
        
        # Decode the token
        token_str = tokenizer.decode([next_token_id])
        
        # Check for EOS tokens (including chat-specific ones)
        eos_token_ids = [tokenizer.eos_token_id]
        # Add additional EOS tokens for chat models (like Llama's <|eot_id|>)
        if hasattr(tokenizer, 'convert_tokens_to_ids'):
            additional_eos = ['<|eot_id|>', '<|end_of_text|>', '</s>']
            for token in additional_eos:
                try:
                    token_id = tokenizer.convert_tokens_to_ids(token)
                    if token_id is not None and token_id != tokenizer.unk_token_id:
                        eos_token_ids.append(token_id)
                except:
                    pass
        
        if next_token_id in eos_token_ids:
            print(f"EOS token detected: {token_str} (id: {next_token_id})")
            break
        
        # Yield token data
        yield {
            "token": token_str,
            "probability": token_prob,
            "rank": token_rank,
            "vocab_size": vocab_size,
            "top_alternatives": top_alternatives
        }
        
        # Append to generated sequence
        generated_ids = torch.cat([generated_ids, torch.tensor([[next_token_id]], device=device)], dim=1)


@app.route('/')
def chat():
    """Chat interface route."""
    return render_template('chat.html')


@app.route('/stream', methods=['POST'])
def stream():
    """SSE endpoint for streaming token generation."""
    try:
        data = request.get_json()
        messages = data.get('messages', [])
        model_name = data.get('model', DEFAULT_MODEL)
        
        if not messages:
            return Response("data: " + json.dumps({"type": "error", "message": "No messages provided"}) + "\n\n", mimetype='text/event-stream')
        
        def generate():
            try:
                # Load model
                model, tokenizer, device = load_model_and_tokenizer(model_name)
                
                # Generate tokens with streaming
                for token_data in generate_streaming_tokens(model, tokenizer, device, messages):
                    # Send token data as SSE
                    yield f"data: {json.dumps({'type': 'token', **token_data})}\n\n"
                
                # Send completion signal
                yield f"data: {json.dumps({'type': 'done'})}\n\n"
                
            except Exception as e:
                print(f"Error during streaming: {e}")
                import traceback
                traceback.print_exc()
                yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
        
        return Response(stream_with_context(generate()), mimetype='text/event-stream')
    
    except Exception as e:
        print(f"Error parsing request: {e}")
        return Response("data: " + json.dumps({"type": "error", "message": f"Invalid request: {str(e)}"}) + "\n\n", mimetype='text/event-stream')


@app.route('/old')
def index():
    """Main route for token probability visualization."""
    # Parse request parameters
    mode = request.args.get('mode', 'file')  # 'file' or 'direct'
    selected_file = request.args.get('file')
    prob_threshold = request.args.get('prob_threshold', default=DEFAULT_PROB_THRESHOLD, type=float)
    threshold_mode = request.args.get('threshold_mode', default='probability', type=str)
    rank_threshold = request.args.get('rank_threshold', default=DEFAULT_RANK_THRESHOLD, type=int)
    max_rank_for_scaling = request.args.get('max_rank_for_scaling', default=DEFAULT_MAX_RANK_SCALING, type=int)
    min_prob_for_scaling = request.args.get('min_prob_for_scaling', default=DEFAULT_MIN_PROB_SCALING, type=float)
    
    # Initialize variables
    token_data = []
    lowest_probs = []
    prompt_text = None
    error = None
    token_input_list = []
    vocab_size = None
    model_name = None
    
    # Scan for available files
    available_files, scan_error = scan_available_files()
    if scan_error:
        error = scan_error
    
    # Check if we have direct analysis data from session
    if mode == 'direct' and 'analysis_data' in session:
        analysis_data = session.get('analysis_data')
        prompt_text = analysis_data.get('prompt')
        token_input_list = analysis_data.get('tokens', [])
        vocab_size = analysis_data.get('vocab_size')
        model_name = analysis_data.get('model_name')
    elif mode == 'file' or not session.get('analysis_data'):
        # File mode - existing logic
        if not error:
            file_to_load, selected_file = determine_file_to_load(available_files, selected_file)
            
            if not file_to_load and selected_file:
                error = f"Selected file '{selected_file}' not found in {OUTPUT_DATA_DIR}."
            elif not file_to_load and not available_files:
                error = f"No JSON files found in {OUTPUT_DATA_DIR}. Run probability_monitor.py first or use New Analysis."
            elif not file_to_load:
                error = None  # Allow showing the new analysis form
            
            # Load and process data if file is available
            if not error and file_to_load:
                prompt_text, token_input_list, vocab_size, model_name, load_error = load_token_data(file_to_load)
                
                if load_error:
                    error = load_error
    
    # Process data if we have token input (from either file or direct analysis)
    if token_input_list and vocab_size and not error:
        # Process token data
        token_data = process_token_data(token_input_list, vocab_size, threshold_mode, max_rank_for_scaling, min_prob_for_scaling)
        
        # Apply threshold filtering
        token_data, lowest_probs = apply_threshold_filtering(
            token_data, threshold_mode, prob_threshold, rank_threshold
        )

    return render_template('index.html', 
                          token_data=token_data, 
                          error=error,
                          prompt_text=prompt_text,
                          lowest_probs=lowest_probs,
                          available_files=available_files,
                          selected_file=selected_file,
                          prob_threshold=prob_threshold,
                          threshold_mode=threshold_mode,
                          rank_threshold=rank_threshold,
                          max_rank_for_scaling=max_rank_for_scaling,
                          min_prob_for_scaling=min_prob_for_scaling,
                          mode=mode,
                          model_name=model_name,
                          default_model=DEFAULT_MODEL)


@app.route('/analyze', methods=['POST'])
def analyze():
    """Process direct prompt and output analysis."""
    try:
        prompt = request.form.get('prompt', '').strip()
        output = request.form.get('output', '').strip()
        model_name = request.form.get('model_name', DEFAULT_MODEL).strip()
        custom_filename = request.form.get('filename', '').strip()
        
        # Validation
        if not prompt:
            flash('Error: Prompt cannot be empty', 'error')
            return redirect(url_for('index'))
        
        if not output:
            flash('Error: Output cannot be empty', 'error')
            return redirect(url_for('index'))
        
        # Load model and calculate probabilities
        print(f'Loading model {model_name}...')
        model, tokenizer, device = load_model_and_tokenizer(model_name)
        
        print('Calculating token probabilities...')
        token_input_list, vocab_size = calculate_token_probabilities(model, tokenizer, device, prompt, output)
        
        # Use custom filename or generate timestamp-based one
        if custom_filename:
            # Sanitize filename
            safe_filename = re.sub(r'[^\w\-_]', '_', custom_filename)
            json_filename = f"{safe_filename}.json"
        else:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            json_filename = f"direct_analysis_{timestamp}.json"
        
        output_json_path = os.path.join(OUTPUT_DATA_DIR, json_filename)
        
        if not os.path.exists(OUTPUT_DATA_DIR):
            os.makedirs(OUTPUT_DATA_DIR)
        
        with open(output_json_path, 'w', encoding='utf-8') as f:
            output_container = {
                "prompt": prompt,
                "vocab_size": vocab_size,
                "model_name": model_name,
                "tokens": token_input_list
            }
            json.dump(output_container, f, ensure_ascii=False, indent=2)
        
        # Store in session for display
        session['analysis_data'] = {
            'prompt': prompt,
            'vocab_size': vocab_size,
            'model_name': model_name,
            'tokens': token_input_list
        }
        
        flash(f'Analysis complete! Saved as {json_filename}', 'success')
        return redirect(url_for('index', mode='direct'))
        
    except Exception as e:
        flash(f'Error during analysis: {str(e)}', 'error')
        return redirect(url_for('index'))


if __name__ == '__main__':
    app.run(debug=True, port=5001)